{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fb3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file PosixPath('/home/shuonan.chen/miniconda3/envs/allensdk/lib/python3.8/site-packages/matplotlib/mpl-data/matplotlibrc'), line 271 ('font.sans-serif: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['code_root', 'data_root', 'package_root'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import json\n",
    "import cv2\n",
    "import os,glob\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('/home/shuonan.chen/scratch_shuonan/code/pons_merfish_pipeline/processing/func')\n",
    "from registerCCF_util import *\n",
    "from prepare_img_utils import *\n",
    "from utils import get_paths\n",
    "\n",
    "def get_record(s):\n",
    "    return {\n",
    "        'filename':s.get('filename'),\n",
    "        'height': s.get('height'),\n",
    "        'width': s.get('width'), \n",
    "        'ox': s.get('anchoring')[0], \n",
    "        'oy': s.get('anchoring')[1],\n",
    "        'oz': s.get('anchoring')[2],\n",
    "        'ux': s.get('anchoring')[3],\n",
    "        'uy': s.get('anchoring')[4],\n",
    "        'uz': s.get('anchoring')[5], \n",
    "        'vx': s.get('anchoring')[6], \n",
    "        'vy': s.get('anchoring')[7],\n",
    "        'vz': s.get('anchoring')[8],\n",
    "    }\n",
    "\n",
    "HOMEDIR = '/allen/aind/scratch/shuonan.chen/code/pons_merfish_pipeline/'\n",
    "paths = get_paths()\n",
    "print(paths.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494e289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MouseC_C2\n",
      "MouseC_C3\n",
      "MouseC_C4\n",
      "MouseC_C5\n",
      "MouseEg_Eg1\n",
      "MouseEg_Eg2\n",
      "MouseEg_Eg3\n",
      "MouseEg_Eg5\n",
      "MouseF_F1\n",
      "MouseF_F2\n",
      "MouseF_F3\n",
      "MouseF_F4\n",
      "MouseZM_ZM0\n",
      "MouseZM_ZM1\n",
      "MouseZM_ZM2\n",
      "MouseZM_ZM3\n",
      "MouseZM_ZM4\n",
      "MouseZM_ZM5p1\n",
      "MouseZM_ZM5p2\n",
      "MouseZM_ZM6p1\n",
      "MouseZM_ZM7p1\n",
      "MouseZM_ZM7p2\n"
     ]
    }
   ],
   "source": [
    "scaling_factor=1/32;\n",
    "all_file_path = '/allen/aind/scratch/shuonan.chen/code/pons_merfish_pipeline/processing/image_xml/*.jpg'\n",
    "for f in glob.glob(all_file_path):\n",
    "    print(os.path.basename(f).split('cell_img_')[1].split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f1c235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MouseC_C2', 'MouseC_C3', 'MouseC_C4', 'MouseC_C5', 'MouseEg_Eg1', 'MouseEg_Eg2', 'MouseEg_Eg3', 'MouseEg_Eg5', 'MouseF_F1', 'MouseF_F2', 'MouseF_F3', 'MouseF_F4', 'MouseZM_ZM0', 'MouseZM_ZM1', 'MouseZM_ZM2', 'MouseZM_ZM3', 'MouseZM_ZM4', 'MouseZM_ZM5p1', 'MouseZM_ZM5p2', 'MouseZM_ZM6p1', 'MouseZM_ZM7p1', 'MouseZM_ZM7p2'] 22\n"
     ]
    }
   ],
   "source": [
    "slicename_all = [os.path.basename(f).split('cell_img_')[1].split('.')[0] for f in glob.glob(all_file_path)]\n",
    "slicename_all.sort()\n",
    "print(slicename_all, len(slicename_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b540c",
   "metadata": {},
   "source": [
    "# batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_flag = True\n",
    "plotting_2d = False\n",
    "plotting_3d = False\n",
    "slicenum = ['1']\n",
    "slicenum=[int(i) for i in slicenum]\n",
    "\n",
    "for name_of_slice in slicename_all:\n",
    "    image_files=glob.glob(HOMEDIR + f'/processing/image_xml/*{name_of_slice}.jpg')\n",
    "    # load your file\n",
    "    flat_name=[]  \n",
    "    for i in image_files:\n",
    "        result=re.search('(.*).jpg', i)\n",
    "        n=result.group(1)+('_nl.flat')\n",
    "        flat_name+=[n]\n",
    "    assert(len(image_files)==1)\n",
    "    im = cv2.imread(image_files[0])\n",
    "    h,w=im.shape[:2]\n",
    "    json_name=image_files \n",
    "    d = {'slicenum': slicenum, 'json_name': json_name,'flat_name':flat_name}\n",
    "    name_df=pd.DataFrame(data=d)\n",
    "    neurons=pd.read_csv(HOMEDIR+f'/processing/filt_neurons_all/filt_neurons_{name_of_slice}.csv')\n",
    "    neurons=neurons[neurons['slice'].isin(slicenum)]\n",
    "    Max=neurons['x'].max()\n",
    "    Min=neurons['y'].max()\n",
    "    neurons['rescale_x']=neurons['x']*scaling_factor\n",
    "    neurons['rescale_y']=neurons['y']*scaling_factor\n",
    "    neurons['rescale_x'] = neurons['rescale_x'].astype(float)+500 # this was accounting for the padding that we created for the image\n",
    "    neurons['rescale_y'] = neurons['rescale_y'].astype(float)+500\n",
    "    \n",
    "    # load visualign results \n",
    "    with open(HOMEDIR+f'/processing/visualign_rez/{name_of_slice}.json') as f:\n",
    "        vafile=json.load(f)\n",
    "    details={s.get('filename'):s for s in vafile['slices']}\n",
    "    assert(name_of_slice in list(details.keys())[0])\n",
    "    \n",
    "    rez = neurons.groupby('slice', group_keys=False).apply(lambda g: get_adjusted_points(g.assign(slice=g.name), name_df,details)).reset_index(drop=True)\n",
    "    neurons[['adjusted_x', 'adjusted_y']]= rez\n",
    "    neurons_nl=neurons.copy()\n",
    "    neurons_nl['clustid'] =neurons_nl['clustid'].astype(str)\n",
    "    \n",
    "    # load quickniii results \n",
    "    f = open(HOMEDIR+f'/processing/quicknii_rez/quicknii_{name_of_slice}.json','r')\n",
    "    data=json.loads(f.read())\n",
    "    anchor=pd.DataFrame.from_records([get_record(s) for s in data['slices']])\n",
    "    f.close()\n",
    "\n",
    "    vox_dfs = []\n",
    "    for slice_num, df in neurons_nl.groupby('slice'):\n",
    "        quicknii_cord = get_quicknii_cord(slice_num, df,name_df,anchor,h,w) # Height and Width of image file\n",
    "        vox_cord = get_vox_cord(quicknii_cord)\n",
    "        vox_dfs += [vox_cord]    \n",
    "    vox_df = pd.concat(vox_dfs)\n",
    "    \n",
    "    if save_flag:\n",
    "        vox_df.to_csv(HOMEDIR+f'/processing/registered_foo/registered_{name_of_slice}.csv',index=False)\n",
    "    \n",
    "    # optional plotting!\n",
    "    if plotting_3d:\n",
    "        plot_df = vox_df.copy()\n",
    "        fig = px.scatter_3d(plot_df, x='y_CCF', y='z_CCF', z='x_CCF', color='clustid')\n",
    "        fig.update_traces(marker=dict(size=.5),selector=dict(mode='markers'))\n",
    "        fig.update_scenes(aspectmode='data')\n",
    "        fig.show()\n",
    "    elif plotting_2d:\n",
    "        fig = px.scatter(plot_df, x='z_CCF', y='y_CCF', color='clustid')\n",
    "        fig.update_traces(marker=dict(size=1),selector=dict(mode='markers'))\n",
    "        fig.update_yaxes(scaleanchor='x', scaleratio=1)\n",
    "        fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6ff00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name_of_slice in slicename_all:\n",
    "    image_files=glob.glob(HOMEDIR + f'/processing/image_xml/*{name_of_slice}.jpg')\n",
    "    # load your file\n",
    "    flat_name=[]  \n",
    "    for i in image_files:\n",
    "        result=re.search('(.*).jpg', i)\n",
    "        n=result.group(1)+('_nl.flat')\n",
    "        flat_name+=[n]\n",
    "    assert(len(image_files)==1)\n",
    "    im = cv2.imread(image_files[0])\n",
    "    h,w=im.shape[:2]\n",
    "    json_name=image_files \n",
    "    d = {'slicenum': slicenum, 'json_name': json_name,'flat_name':flat_name}\n",
    "    name_df=pd.DataFrame(data=d)\n",
    "    neurons=pd.read_csv(HOMEDIR+f'/processing/filt_neurons_all/filt_neurons_{name_of_slice}.csv')\n",
    "    neurons=neurons[neurons['slice'].isin(slicenum)]\n",
    "    Max=neurons['x'].max()\n",
    "    Min=neurons['y'].max()\n",
    "    neurons['rescale_x']=neurons['x']*scaling_factor\n",
    "    neurons['rescale_y']=neurons['y']*scaling_factor\n",
    "    neurons['rescale_x'] = neurons['rescale_x'].astype(float)+500 # this was accounting for the padding that we created for the image\n",
    "    neurons['rescale_y'] = neurons['rescale_y'].astype(float)+500\n",
    "    \n",
    "    # load visualign results \n",
    "    with open(HOMEDIR+f'/processing/visualign_rez/{name_of_slice}.json') as f:\n",
    "        vafile=json.load(f)\n",
    "    details={s.get('filename'):s for s in vafile['slices']}\n",
    "    assert(name_of_slice in list(details.keys())[0])\n",
    "    \n",
    "    rez = neurons.groupby('slice', group_keys=False).apply(lambda g: get_adjusted_points(g.assign(slice=g.name), name_df,details)).reset_index(drop=True)\n",
    "    neurons[['adjusted_x', 'adjusted_y']]= rez\n",
    "    neurons_nl=neurons.copy()\n",
    "    neurons_nl['clustid'] =neurons_nl['clustid'].astype(str)\n",
    "    \n",
    "    # load quickniii results \n",
    "    f = open(HOMEDIR+f'/processing/quicknii_rez/quicknii_{name_of_slice}.json','r')\n",
    "    data=json.loads(f.read())\n",
    "    anchor=pd.DataFrame.from_records([get_record(s) for s in data['slices']])\n",
    "    f.close()\n",
    "\n",
    "    vox_dfs = []\n",
    "    for slice_num, df in neurons_nl.groupby('slice'):\n",
    "        quicknii_cord = get_quicknii_cord(slice_num, df,name_df,anchor,h,w) # Height and Width of image file\n",
    "        vox_cord = get_vox_cord(quicknii_cord)\n",
    "        vox_dfs += [vox_cord]    \n",
    "    vox_df = pd.concat(vox_dfs)\n",
    "    \n",
    "    if save_flag:\n",
    "        vox_df.to_csv(HOMEDIR+f'/processing/registered_foo/registered_{name_of_slice}.csv',index=False)\n",
    "    \n",
    "    # optional plotting!\n",
    "    if plotting_3d:\n",
    "        plot_df = vox_df.copy()\n",
    "        fig = px.scatter_3d(plot_df, x='y_CCF', y='z_CCF', z='x_CCF', color='clustid')\n",
    "        fig.update_traces(marker=dict(size=.5),selector=dict(mode='markers'))\n",
    "        fig.update_scenes(aspectmode='data')\n",
    "        fig.show()\n",
    "    elif plotting_2d:\n",
    "        fig = px.scatter(plot_df, x='z_CCF', y='y_CCF', color='clustid')\n",
    "        fig.update_traces(marker=dict(size=1),selector=dict(mode='markers'))\n",
    "        fig.update_yaxes(scaleanchor='x', scaleratio=1)\n",
    "        fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049374cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b78bb00",
   "metadata": {},
   "source": [
    "# plot the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_df = vox_df.copy()\n",
    "fig = px.scatter_3d(plot_df, x='y_CCF', y='z_CCF', z='x_CCF', color='clustid')\n",
    "fig.update_traces(marker=dict(size=.5),\n",
    "                  selector=dict(mode='markers'))\n",
    "fig.update_scenes(aspectmode='data')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbd24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "590e9842",
   "metadata": {},
   "source": [
    "# plot some more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5819b141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786ce935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c97ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8220b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allensdk",
   "language": "python",
   "name": "allensdk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
